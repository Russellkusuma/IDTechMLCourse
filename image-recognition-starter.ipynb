{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Image Recognition\n",
    "===\n",
    "\n",
    "This notebook will create a convolutional neural network to classify images in either the mnist or cifar-10 datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ImportError",
     "evalue": "No module named 'tensorflow'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-2f6e48e7b17e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Tensorflow and numpy to create the neural network\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# Matplotlib to plot info to show our results\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "# Tensorflow and numpy to create the neural network\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Matplotlib to plot info to show our results\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# OS to load files and save checkpoints\n",
    "import os\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"lol\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the data\n",
    "---\n",
    "\n",
    "This code will load the dataset that you'll use to train and test the model.\n",
    "\n",
    "The code provided will load the mnist or cifar data from files, you'll need to add the code that processes it into a format your neural network can use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MNIST\n",
    "---\n",
    "\n",
    "Run this cell to load mnist data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST-data/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST-data/train-labels-idx1-ubyte.gz\nExtracting MNIST-data/t10k-images-idx3-ubyte.gz\nExtracting MNIST-data/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(55000, 784)\n(55000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "# Load MNIST data from tf examples\n",
    "\n",
    "image_height = 28\n",
    "image_width = 28\n",
    "\n",
    "color_channels = 1\n",
    "\n",
    "model_name = \"mnist\"\n",
    "\n",
    "mnist = tf.contrib.learn.datasets.load_dataset(\"mnist\")\n",
    "\n",
    "train_data = mnist.train.images\n",
    "train_labels = np.asarray(mnist.train.labels, dtype=np.int32)\n",
    "\n",
    "eval_data = mnist.test.images\n",
    "eval_labels = np.asarray(mnist.test.labels, dtype=np.int32)\n",
    "\n",
    "category_names = list(map(str, range(10)))\n",
    "\n",
    "# TODO: Process mnist data\n",
    "print(train_data.shape)\n",
    "\n",
    "train_data = np.reshape(train_data, (-1, image_height, image_width, color_channels))\n",
    "\n",
    "print(train_data.shape)\n",
    "\n",
    "eval_data = np.reshape(eval_data, (-1, image_height, image_width, color_channels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CIFAR-10\n",
    "---\n",
    "\n",
    "Run this cell to load cifar-10 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load cifar data from file\n",
    "# image_height = 32\n",
    "# image_width = 32\n",
    "# \n",
    "# color_channels = 3\n",
    "# \n",
    "# model_name = \"cifar\"\n",
    "# \n",
    "# def unpickle(file):\n",
    "#    import pickle\n",
    "#    with open(file, 'rb') as fo:\n",
    "#        dict = pickle.load(fo, encoding='bytes')\n",
    "#    return dict\n",
    "# cifar_path = './Desktop/Russell_PycharmML/cifar-10-batches-py/'\n",
    "# \n",
    "# train_data = np.array([])\n",
    "# train_labels = np.array([])\n",
    "# \n",
    "# # Load all the data batches.\n",
    "# for i in range(1, 6):\n",
    "#    data_batch = unpickle(cifar_path + 'data_batch_' + str(i))\n",
    "#    train_data = np.append(train_data, data_batch[b'data'])\n",
    "#    train_labels = np.append(train_labels, data_batch[b'labels'])\n",
    "# \n",
    "# # Load the eval batch.\n",
    "# eval_batch = unpickle(cifar_path + 'test_batch')\n",
    "# \n",
    "# eval_data = eval_batch[b'data']\n",
    "# eval_labels = eval_batch[b'labels']\n",
    "# \n",
    "# # Load the english category names.\n",
    "# category_names_bytes = unpickle(cifar_path + 'batches.meta')[b'label_names']\n",
    "# category_names = list(map(lambda x: x.decode(\"utf-8\"), category_names_bytes))\n",
    "# \n",
    "# # TODO: Process Cifar data\n",
    "# def process_data(data):\n",
    "#     float_data = np.array(data, dtype=float) / 255.0\n",
    "#      \n",
    "#     reshaped_data = np.reshape(float_data, (-1, color_channels, image_height, image_width))\n",
    "#     \n",
    "#     # The incorrect image\n",
    "#      \n",
    "#     transposed_data = np.transpose(reshaped_data, [0, 2, 3, 1])\n",
    "#     \n",
    "#     return transposed_data\n",
    "#  \n",
    "# train_data = process_data(train_data)\n",
    "# \n",
    "# eval_data = process_data(eval_data)\n",
    "# \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the data is processed, you have a few variables for the data itself and info about its shape:\n",
    "\n",
    "### Model Info\n",
    "\n",
    "- **image_height, image_width** - The height and width of the processed images\n",
    "- **color_channels** - the number of color channels in the image. This will be either 1 for grayscale or 3 for rgb.\n",
    "- **model_name** - either \"cifar\" or \"mnist\" - if you need to handle anything differently based on the model, check this variable.\n",
    "- **category_names** - strings for each category name (used to print out labels when testing results)\n",
    "\n",
    "### Training Data\n",
    "\n",
    "- **train_data** - the training data images\n",
    "- **train_labels** - the labels for the training data - the \"answer key\"\n",
    "\n",
    "### Evaluation Data\n",
    "\n",
    "- **eval_data** - Image data for evaluation. A different set of images to test your network's effectiveness.\n",
    "- **eval_labels** - the answer key for evaluation data.\n",
    "\n",
    "Building the Neural Network Model\n",
    "--\n",
    "\n",
    "Next, you'll build a neural network with the following architecture:\n",
    "\n",
    "- An input placeholder that takes one or more images.\n",
    "- 1st Convolutional layer with 32 filters and a kernel size of 5x5 and same padding\n",
    "- 1st Pooling layer with a 2x2 pool size and stride of 2\n",
    "- 2nd Convolutional layer with 64 filters and a kernel size of 5x5 and same padding\n",
    "- 2nd Pooling layer with a 2x2 pool size and stride of 2\n",
    "- Flatten the pooling layer\n",
    "- A fully connected layer with 1024 units\n",
    "- A dropout layer with a rate of 0.4\n",
    "- An output layer with an output size equal to the number of labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: The neural network\n",
    "class ConvNet:\n",
    "    \n",
    "    def __init__(self, image_height, image_width, channels, num_classes):\n",
    "        #^ This is initializing the image with each characteristics\n",
    "        #classes is basicly the number of possible classes for the network\n",
    "        \n",
    "        self.input_layer = tf.placeholder(dtype=tf.float32, shape=[None, image_height, image_width, channels], name=\"inputs\")\n",
    "        print(self.input_layer.shape)\n",
    "        #^ input layer is basicly a placeholder tensor for image input\n",
    "        # Printing .shape is important to understand what its looking at\n",
    "        \n",
    "        conv_layer_1 = tf.layers.conv2d(self.input_layer, filters = 32, kernel_size=[5,5], padding=\"same\", activation=tf.nn.relu)\n",
    "        #^Declare first convolutional layer (Creates convultional layer with layers.conv2d)\n",
    "        print(conv_layer_1.shape)\n",
    "        \n",
    "        pooling_layer_1 = tf.layers.max_pooling2d(conv_layer_1, pool_size=[2,2], strides=2)\n",
    "        #^Declare first pooling layer (Creates pooling layer with layers.max_pooling2d)\n",
    "        print(pooling_layer_1.shape)\n",
    "        \n",
    "        # TODO: Add a conv2d and pooling layer (same thing as the first)\n",
    "        conv_layer_2 = tf.layers.conv2d(self.input_layer, filters = 32, kernel_size=[5,5], padding=\"same\", activation=tf.nn.relu)\n",
    "        print(conv_layer_2.shape)\n",
    "        \n",
    "        pooling_layer_2 = tf.layers.max_pooling2d(conv_layer_1, pool_size=[2,2], strides=2)\n",
    "        print(pooling_layer_2.shape)\n",
    "        \n",
    "        flattened_pooling = tf.layers.flatten(pooling_layer_2)\n",
    "        #^Declare dense layer to make the final classification\n",
    "        dense_layer = tf.layers.dense(flattened_pooling, 1024, activation=tf.nn.relu)\n",
    "        print(dense_layer.shape)\n",
    "        \n",
    "        dropout = tf.layers.dropout(dense_layer, rate=0.4, training=True)\n",
    "        #^Drop out layer to create variation in neutron growth\n",
    "        \n",
    "        outputs = tf.layers.dense(dropout, num_classes)\n",
    "        #^Output layer another dense layer\n",
    "        print(outputs.shape)\n",
    "        \n",
    "        self.choice = tf.argmax(outputs, axis=1)\n",
    "        self.probability = tf.nn.softmax(outputs)\n",
    "        #^ computer makes a choice\n",
    "        \n",
    "        self.labels = tf.placeholder(dtype=tf.float32, name=\"labels\")\n",
    "        self.accuracy, self.accuracy_op = tf.metrics.accuracy(self.labels, self.choice)\n",
    "        #Labels/Accuracy\n",
    "        \n",
    "        one_hot_labels = tf.one_hot(indices=tf.cast(self.labels, dtype=tf.int32), depth=num_classes)     \n",
    "        self.loss = tf.losses.softmax_cross_entropy(onehot_labels=one_hot_labels, logits=outputs)\n",
    "        #^I S  T H I S  L O S S\n",
    "        \n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate=1e-2)\n",
    "        self.train_operation = optimizer.minimize(loss=self.loss, global_step=tf.train.get_global_step())\n",
    "        #The gradient Descent Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Training Process\n",
    "---\n",
    "\n",
    "The cells below will set up and run the training process.\n",
    "\n",
    "- Set up initial values for batch size, training length.\n",
    "- Process data into batched datasets to feed into the network.\n",
    "- Run through batches of training data, update weights, save checkpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: initialize variables\n",
    "training_steps = 10000\n",
    "batch_size = 16\n",
    "#^ Number of images sent at each step of training\n",
    "path = \"./\" + model_name + \"-cnn/\"\n",
    "performance_graph = np.array([])\n",
    "load_checkpoint = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 28, 28, 1)\n(?, 28, 28, 32)\n(?, 14, 14, 32)\n(?, 28, 28, 32)\n(?, 14, 14, 32)\n(?, 1024)\n(?, 10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy at step 1000: 0.8176823\nSaving checkpoint\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy at step 2000: 0.8649113\nSaving checkpoint\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy at step 3000: 0.88699603\nSaving checkpoint\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy at step 4000: 0.9016998\nSaving checkpoint\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy at step 5000: 0.9117338\nSaving checkpoint\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy at step 6000: 0.9188818\nSaving checkpoint\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy at step 7000: 0.92473215\nSaving checkpoint\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy at step 8000: 0.9295547\nSaving checkpoint\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy at step 9000: 0.93377084\nSaving checkpoint\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving final checkpoint for training session.\n"
     ]
    }
   ],
   "source": [
    "# TODO: implement the training loop\n",
    "tf.reset_default_graph()\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((train_data, train_labels))\n",
    "\n",
    "dataset = dataset.shuffle(buffer_size=train_labels.shape[0])\n",
    "dataset = dataset.batch(batch_size)\n",
    "dataset = dataset.repeat()\n",
    "#Loop through the data set?\n",
    "dataset_iterator = dataset.make_initializable_iterator()\n",
    "next_element = dataset_iterator.get_next()\n",
    " \n",
    "#Set up the neural network\n",
    "cnn = ConvNet(image_height, image_width, color_channels,10)\n",
    " \n",
    "#Saver to keep neural network progress when leaving\n",
    "saver = tf.train.Saver(max_to_keep=2)\n",
    " \n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "    \n",
    "#Saving the variables\n",
    "with tf.Session() as sess:\n",
    "     \n",
    "    if load_checkpoint:\n",
    "        checkpoint = tf.train.get_checkpoint_state(path)\n",
    "        saver.restore(sess, checkpoint.model_checkpoint_path)\n",
    "    else:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "     \n",
    "    sess.run(tf.local_variables_initializer())\n",
    "    sess.run(dataset_iterator.initializer)\n",
    "    for step in range(training_steps):\n",
    "        current_batch = sess.run(next_element)\n",
    "         \n",
    "        batch_inputs = current_batch[0]\n",
    "        batch_labels = current_batch[1]\n",
    "         \n",
    "        sess.run((cnn.train_operation, cnn.accuracy_op), feed_dict={ cnn.input_layer:batch_inputs, cnn.labels:batch_labels})\n",
    "        \n",
    "                 \n",
    "        if step % 10 == 0:\n",
    "            performance_graph = np.append(performance_graph, sess.run(cnn.accuracy))\n",
    "        \n",
    "        if step % 1000 == 0 and step > 0:\n",
    "            current_acc = sess.run(cnn.accuracy)\n",
    "             \n",
    "            print(\"Accuracy at step \" + str(step) + \": \" + str(current_acc))\n",
    "            print(\"Saving checkpoint\")\n",
    "            saver.save(sess, path + model_name, step)\n",
    "         \n",
    "    print(\"Saving final checkpoint for training session.\")\n",
    "    saver.save(sess, path + model_name, step)\n",
    "    print(\"Final Accuracy: \" + str(current_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating Performance\n",
    "---\n",
    "\n",
    "These cells will evaluate the performance of your network!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0,0.5,'Accuracy')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAD8CAYAAABzTgP2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f4e6c5c8b70>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEKCAYAAAAMzhLIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nUQAArCgKAIAVRQEAsKIoAABW/wf9BhsU7O9eWQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f4e7a3f8c50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO: Display graph of performance over time\n",
    "plt.plot(performance_graph)\n",
    "plt.figure().set_facecolor('white')\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.ylabel(\"Accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Run through the evaluation data set, check accuracy of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Get a random set of images and make guesses for each"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "3.5.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}